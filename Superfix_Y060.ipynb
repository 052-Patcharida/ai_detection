{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "tpu1vmV38",
      "dataSources": [
        {
          "sourceId": 111865,
          "databundleVersionId": 13328006,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31091,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Superfix-Y060",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/052-Patcharida/ai_detection/blob/data-sci-mirt/Superfix_Y060.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Dyloj82x48M7"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "mitr_phol_gen_ai_hackathon_path = kagglehub.competition_download('mitr-phol-gen-ai-hackathon')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "kjR9eE8_48M8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/kaggle/input/mitr-phol-gen-ai-hackathon/train.csv')\n",
        "print(train_df)\n",
        "print(train_df['bu_categories'].value_counts())\n",
        "print(train_df['action_non'].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "id": "bx7xKRQW48M9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet pdfplumber\n",
        "!pip install --quiet fitz\n",
        "!pip install --quiet pythainlp\n",
        "!pip install --upgrade scikit-learn imbalanced-learn --quiet"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T11:36:16.15896Z",
          "iopub.execute_input": "2025-08-09T11:36:16.159271Z",
          "iopub.status.idle": "2025-08-09T11:36:48.172084Z",
          "shell.execute_reply.started": "2025-08-09T11:36:16.159246Z",
          "shell.execute_reply": "2025-08-09T11:36:48.170968Z"
        },
        "id": "QeEwYgmM48M9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pdfplumber\n",
        "import re\n",
        "\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.corpus.common import thai_stopwords\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
        "# ------------------------------\n",
        "train_df = pd.read_csv('/kaggle/input/mitr-phol-gen-ai-hackathon/train.csv')\n",
        "train_df = train_df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡∏≠‡πà‡∏≤‡∏ô PDF\n",
        "# ------------------------------\n",
        "base_path = '/kaggle/input/mitr-phol-gen-ai-hackathon/train_docs/'\n",
        "\n",
        "def extract_text_from_pdf(pdf_name):\n",
        "    pdf_path = base_path + pdf_name\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_name}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "train_df['text'] = train_df['pdf_name'].apply(extract_text_from_pdf)\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
        "# ------------------------------\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\u0E00-\\u0E7Fa-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "train_df['clean_text'] = train_df['text'].apply(clean_text)\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ Tokenizer + Stopwords\n",
        "# ------------------------------\n",
        "stopwords = set(thai_stopwords())\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    tokens = word_tokenize(text, engine='newmm')\n",
        "    return [t for t in tokens if t not in stopwords and len(t) > 1]\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡∏™‡∏£‡πâ‡∏≤‡∏á TF-IDF ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
        "# ------------------------------\n",
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=clean_and_tokenize,\n",
        "    token_pattern=None,\n",
        "    ngram_range=(1, 3),   # ‡πÄ‡∏î‡∏¥‡∏° 1,2 ‚Üí ‡∏•‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏õ‡πá‡∏ô 1,3\n",
        "    max_features=50000,   # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≥‡∏ô‡∏ß‡∏ô feature\n",
        "    min_df=2,             # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô\n",
        "    max_df=0.9            # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏ö‡πà‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô\n",
        ")\n",
        "\n",
        "X = vectorizer.fit_transform(train_df['clean_text'])\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/val\n",
        "# ------------------------------\n",
        "X_train, X_val, y_train_action, y_val_action, y_train_cat, y_val_cat = train_test_split(\n",
        "    X,\n",
        "    train_df['action_non'],\n",
        "    train_df['bu_categories'],\n",
        "    test_size=0.05,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡πÇ‡∏°‡πÄ‡∏î‡∏• Logistic Regression (action)\n",
        "# ------------------------------\n",
        "model_action = LogisticRegression(\n",
        "    max_iter=1000,          # ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÑ‡∏î‡πâ‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô\n",
        "    class_weight='balanced',\n",
        "    solver='saga',          # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö L1/L2 regularization\n",
        "    penalty='l2',           # ‡∏ä‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ overfit\n",
        "    n_jobs=-1               # ‡πÉ‡∏ä‡πâ‡∏ó‡∏∏‡∏Å core CPU\n",
        ")\n",
        "model_action.fit(X_train, y_train_action)\n",
        "y_pred_action = model_action.predict(X_val)\n",
        "\n",
        "print(\"\\nüìä Classification Report (ACTION):\")\n",
        "print(classification_report(y_val_action, y_pred_action))\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡πÇ‡∏°‡πÄ‡∏î‡∏• Logistic Regression (category)\n",
        "# ------------------------------\n",
        "model_cat = LogisticRegression(max_iter=300, class_weight='balanced')\n",
        "model_cat.fit(X_train, y_train_cat)\n",
        "y_pred_cat = model_cat.predict(X_val)\n",
        "\n",
        "print(\"\\nüìä Classification Report (CATEGORY):\")\n",
        "print(classification_report(y_val_cat, y_pred_cat))\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ TF-IDF Vocabulary ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
        "# ------------------------------\n",
        "print(\"\\nüî§ TF-IDF Vocabulary (‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô):\")\n",
        "print(vectorizer.get_feature_names_out()[:50])\n",
        "\n",
        "print(\"\\nüìù Clean Text ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á:\")\n",
        "print(train_df['clean_text'].head())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-09T11:37:25.952511Z",
          "iopub.execute_input": "2025-08-09T11:37:25.952851Z",
          "iopub.status.idle": "2025-08-09T11:45:21.030616Z",
          "shell.execute_reply.started": "2025-08-09T11:37:25.952824Z",
          "shell.execute_reply": "2025-08-09T11:45:21.029392Z"
        },
        "id": "k4cuZ_Ys48M9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# üîπ ‡πÇ‡∏´‡∏•‡∏î submission.csv ‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• test\n",
        "# ------------------------------\n",
        "test_df = pd.read_csv('/kaggle/input/mitr-phol-gen-ai-hackathon/submission.csv')\n",
        "\n",
        "# üîπ ‡∏≠‡πà‡∏≤‡∏ô PDF ‡∏à‡∏≤‡∏Å test_docs\n",
        "test_base_path = '/kaggle/input/mitr-phol-gen-ai-hackathon/test_docs/'\n",
        "\n",
        "def extract_text_from_pdf_test(pdf_name):\n",
        "    pdf_path = test_base_path + pdf_name\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            text = \"\"\n",
        "            for page in pdf.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_name}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "test_df['text'] = test_df['pdf_name'].apply(extract_text_from_pdf_test)\n",
        "test_df['clean_text'] = test_df['text'].apply(clean_text)\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô TF-IDF ‡∏î‡πâ‡∏ß‡∏¢ vectorizer ‡πÄ‡∏î‡∏¥‡∏°\n",
        "# ------------------------------\n",
        "X_test = vectorizer.transform(test_df['clean_text'])\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ó‡∏£‡∏ô‡πÑ‡∏ß‡πâ\n",
        "# ------------------------------\n",
        "pred_action = model_action.predict(X_test)\n",
        "pred_cat = model_cat.predict(X_test)\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡∏£‡∏ß‡∏°‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
        "# ------------------------------\n",
        "final_pred = [f\"{cat}_{act}\"\n",
        "              for cat, act in zip(pred_cat, pred_action)]\n",
        "\n",
        "test_df['result'] = final_pred\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á\n",
        "# ------------------------------\n",
        "print(\"\\nüìÑ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ú‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å submission.csv\")\n",
        "print(test_df[['pdf_name', 'result']].head())\n",
        "\n",
        "# ------------------------------\n",
        "# üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏™‡πà‡∏á)\n",
        "# ------------------------------\n",
        "test_df[['pdf_name', 'result']].to_csv(\"submission.csv\", index=False)\n",
        "print(\"\\n‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏õ‡πá‡∏ô submission.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "9s26aHvy48M-"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}